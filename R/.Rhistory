producers[[movie_title]] <- producer_names
} else {
producers[[movie_title]] <- NA  #
}
}
library(XML)
library(curl)
states = readHTMLTable(readLines(curl("https://simple.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_area")), stringsAsFactors = FALSE)
states = states[[1]]
states = states[3:52,]
states$V3 =  gsub(",", "", states$V3)
head(states)
farm = read.csv("farm.csv")
head(farm)
#sort(unique(farm$state))
#sort(unique(states$V1))
#WHY IS THERE LEADING SPACE THIS TOOK FOREVER LOL
farm$state <- gsub("[^[:alpha:]]", "", farm$state)
states$V1 <- gsub("[^[:alpha:]]", "", states$V1)
area = merge(farm, states, by.x="state", by.y="V1")
area = area[,c("state", "sq.miles", "V3")]
names(area) = c("state", "farm", "land")
head(area)
area$land = as.integer(area$land)
head(area)
plot(area$land, area$farm, xlab="Land Area (sq miles)", ylab="Farm Area (sq miles)",
main="Farm vs. Land Area") #Not sure why xlabels are so weird
m_full = lm(farm ~ land, data=area)
#identify(resid(m_full))
m_full = lm(farm ~ land, data=area)
plot(area$land, area$farm, xlab="Land Area (sq miles)", ylab="Farm Area (sq miles)",
main="Farm vs. Land Area") #Not sure why xlabels are so weird
abline(m_full, col="blue")
m_no_al = lm(farm ~ land, data=area[-2,])
abline(m_no_al, col="red")
plot(resid(m_full), ylab='Residuals', xlab='Index', main='Residuals')
r.jack = numeric(length=50)
for (i in 1:50){
model_i = lm(farm ~ land, data=area[-i,])
predicted_farm_i = predict(model_i, newdata=area[i, , drop=FALSE])
r.jack[i] = area$farm[i] - predicted_farm_i
}
plot(r.jack, type='h', lwd=2, col='blue',
main='Residuals', xlab='Index', ylab='Residual',
ylim=c(min(r.jack), max(r.jack)))
rm(list=ls())
if (!require("XML")) {
install.packages("XML") # do this once per lifetime
stopifnot(require("XML")) # do this once per session
}
if (!require("curl")) {
install.packages("curl") # do this once per lifetime
stopifnot(require("curl")) # do this once per session
}
if (!require("rvest")) {
install.packages("rvest") # do this once per lifetime
stopifnot(require("rvest")) # do this once per session
}
url = "https://www.imdb.com/title/tt0111161/fullcredits" # The Shawshank Redemption No. 1, https link
tables = url %>% read_html() %>% html_elements("table")
rm(list=ls())
library(rvest)
library(stringr)
#URL and link class for getting the top movie links
url = "https://www.imdb.com/chart/top"
link.class = ".ipc-title-link-wrapper"
#Links using rvest
links = url %>%
read_html() %>%
html_elements(link.class) %>%
html_attr("href")
#Cleaning up link list
links = links[1:250]
links = sapply(links, function(link) {
str_extract(link, "/title/tt[0-9]+")
})
scrape_producers = function(movie_url) {
full_credits_url = paste("https://www.imdb.com", movie_url, "/fullcredits", sep="", collapse=NULL)
page = read_html(full_credits_url)
#Used a stackoverflow post to help me with this xpath method
producers_names = page %>%
html_elements(xpath = "//h4[@name='producer']/following-sibling::table[1]//td[@class='name']") %>%
html_text(trim = TRUE)
return(producers_names)
}
#Applying the function to every link
producers_list = list()
for (link in links) {
movie_id = str_extract(link, "tt[0-9]+")
producers = scrape_producers(link)
producers_list[[movie_id]] = producers
}
#Unlist and table the producers
all_producers = unlist(producers_list)
producer_counts = table(all_producers)
#Top 5
(top_producers = sort(producer_counts, decreasing = TRUE)[1:5])
rm(list = ls())
if (!require("rvest")) {
install.packages("rvest")
stopifnot(require("rvest"))
}
library(tidyverse)
url <- "https://simple.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_area"
tables <- url %>%
read_html() %>%
html_elements("table") %>%
html_table()
state_table <- tables[[1]]
land_columns <- grep("Land area", colnames(state_table), ignore.case = TRUE)
state_table <- state_table[, c(1, land_columns)]
colnames(state_table) = state_table[1, ]
state_df <- state_table[2:51, ]
rownames(state_df) <- NULL
percentage_cols <- sapply(state_df, function(x) any(grepl("%", x)))
state_df <- state_df[, !percentage_cols]
state_df[, -1] <- lapply(state_df[, -1], function(x) {
as.numeric(gsub(",", "", x))
})
farm_data = read.csv("farm.csv", header = TRUE)
new_df = state_df[, c('Statefederal district or territory', 'sq mi')]
colnames(new_df) = c('state', 'land')
colnames(farm_data) = c('state', 'farm')
area = merge(new_df, farm_data, by = 'state')
colnames(area) = c('state', 'land', 'farm')
y = area$farm
x = area$land
plot(x, y, main = 'Scatter Plot for Farm Area vs Land Area',
xlab = 'Land Area',
ylab = 'Farm Area')
indices = identify(x, y, tolerance = 1, n = 2)
#indices = identify(x, y, tolerance = 1, n = 2)
#outlier1 = indices[1]
#outlier2 = indices[2]
outlier1 = 2
outlier2 = 43
regression_line = lm(farm ~ land, area)
plot(x,y)
abline(regression_line)
area_wo_alaska = area[area$state != 'Alaska', ]
regression_line_wo_alaska = lm(farm ~ land, area_wo_alaska)
abline(regression_line_wo_alaska, col = 'red')
farm.lm = lm(farm ~ land, area)
farm.res = resid(farm.lm)
plot(area$land, farm.res, ylab = 'Residuals', xlab = 'Land sq mi', main = 'Residuals for Original Model')
r.jack = as.numeric()
n = 50
for (i in 1:n) {
temp_w_observation_removed = area[area$state !=area$state[i], ]
observation_removed_index = area[area$state == area$state[i], ]
temp_w_observation_removed_reg = lm(farm ~ land, temp_w_observation_removed)
predicted_val = predict(temp_w_observation_removed_reg, newdata = data.frame(land = observation_removed_index$land))
jack_val = abs(observation_removed_index$farm - predicted_val)
r.jack[i] = jack_val
}
plot(regression_line$fitted.values, r.jack,
xlab = 'Fitted Values',
ylab = 'Jackknife R Residuals',
main = 'Jackknife Residual Plot')
rm(list = ls())
if (!require("rvest")) {
install.packages("rvest")
stopifnot(require("rvest"))
}
base_link = "https://www.imdb.com"
url = "https://www.imdb.com/chart/top"
link.class = ".ipc-title-link-wrapper"
links = url %>% read_html() %>% html_elements(link.class) %>% .[1:250] %>% html_attr("href")
cleaned_links = gsub("\\?.*$", "", links)
general_links = paste0(base_link, cleaned_links)
extended_link = "fullcredits/"
full_url = paste0(general_links, extended_link)
producers = list()
tables = list(length = 250)
for (i in 1:250) {
movie_title = full_url[i] %>%
read_html() %>%
html_element(".parent") %>%
html_text(trim = TRUE)
table.class = '.simpleTable.simpleCreditsTable'
tables = full_url[i] %>%
read_html() %>%
html_elements(table.class) %>%
html_table()
producer = as.data.frame(tables[3])
producers[[i]] = producers$X1
}
producers_unlisted = unlist(producers)
name_table = table(producers_unlisted)
name_table = sort(name_table, decreasing = TRUE)
print(name_table[1:5])
url = "https://simple.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_area"
tables = url %>% read_html() %>% html_elements("table") %>% html_table()
state.table = tables[[1]]
state.table = state.table[-1, ]
state.table = state.table[ , c(1, 6)]
state.table = state.table[c(1:50), ]
names(state.table) <- c("state", "sq.miles")
state.table$sq.miles = as.numeric(gsub(",", "", state.table$sq.miles))
farm.areas = read.csv("farm.csv")
state.table = state.table[order(state.table$state), ]
area = merge(farm.areas, state.table, "state")
names(area) = c("state", "farm", "land")
plot(x = area$land, y = area$farm)
lout = subset(area, land == max(area$land))
fout = subset(area, farm == max(area$farm))
lout; fout
regression = lm(farm ~land, area)
plot(x = area$land, y = area$farm)
y.intercept = regression$coefficients[1]
slope = regression$coefficients[2]
abline(a = y.intercept, b = slope, col = "red")
no.alaska = subset(area, state != "Alaska")
new.reg = lm(farm ~ land, no.alaska)
plot(x = no.alaska$land, y = no.alaska$farm)
y = new.reg$coefficients[1]
s = new.reg$coefficients[2]
abline(a = y, b = s, col = "red")
plot(x = regression$fitted.values, y = regression$residuals)
r.jack = as.numeric()
for (i in unique(area$state)) {
temp.subset = subset(area, state != i)
removed = subset(area, state == i)
temp.regression = lm(farm ~ land, temp_subset)
prediction = predict(temp.regression, newdata = data.frame(land = removed$land))
jack.value = abs(removed$farm - prediction)
r.jack[i] = jack.value
}
rm(list = ls())
if (!require("rvest")) {
install.packages("rvest")
stopifnot(require("rvest"))
}
url = "https://www.imdb.com/chart/top"
link.class = ".ipc-title-link-wrapper" # link class name, found from its HTML source code
links = url %>% read_html() %>% html_elements(link.class) %>% .[1:250] %>% html_attr("href")
links = sub(pattern = "*.?ref_=chttp_t_.*", replacement = "fullcredits/", x = links)
f.links = sub(pattern = "^.", replacement = "https://www.imdb.com/", x = links)
title.links = ".ipc-title__text"
movie.titles = url %>% read_html() %>% html_elements(title.links) %>% html_text2() %>%
gsub(pattern = "(.*)\\n\\n.*", replacement = "\\1", x = .) %>% .[3:252]
producer = list()
producer.name = list()
tables = list(length = 250)
for (i in 1:length(final.links)) {
tables[[i]] = read_html(f.links[i])
producer[[i]] = (tables[[i]][4])
data.frame = as.data.frame(producer[[i]])
producer.name[[i]] = as.vector(data.frame)
}
# Load necessary libraries
library(CropScapeR)
library(rgdal)
# First, install the remotes package if it's not already installed
install.packages("remotes")
# Load necessary libraries
library(CropScapeR)
# Then, use remotes to install CropScapeR from GitHub
remotes::install_github("jbrown1093/CropScapeR")
library(CropScapeR)
# Load necessary libraries
library(httr)  # For making HTTP requests
library(jsonlite)  # For parsing JSON
# Define the API URL for fetching CDL data (Crop Data Layer)
# Using GetCDLStat for getting data by year and bounding box (coordinates)
url <- "https://nassgeodata.gmu.edu/axis2/services/CDLService/GetCDLStat"
# Define the parameters for the request (year, longitude, latitude in bbox format)
params <- list(
year = 2007,  # Define the year you want to retrieve data for
bbox = "-89.593554,44.018333,-89.593554,44.018333",  # Bounding box for your location
format = "json"  # Response format
)
# Make the request to the API
response <- GET(url, query = params)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
crop_data <- content(response, "parsed", simplifyVector = TRUE)
# Print the resulting data
print(crop_data)
# Optionally, you can save the data to a CSV file
write.csv(crop_data, "potato_data_2007.csv")
} else {
print(paste("Error fetching CDL data:", status_code(response)))
}
# Load necessary libraries
library(httr)  # For making HTTP requests
library(jsonlite)  # For parsing JSON
# Step 1: Define the API URL for fetching CDL data (Crop Data Layer)
url <- "https://nassgeodata.gmu.edu/axis2/services/CDLService/GetCDLStat"
# Step 2: Define the parameters for the request
params <- list(
year = 2007,  # Define the year you want to retrieve data for
bbox = "-89.592554,44.018333,-89.592554,44.018333",  # Bounding box for your location (single point)
format = "json"  # Response format
)
# Print the constructed URL with parameters
print(paste("Request URL:", url))
print(paste("Parameters:", toString(params)))
# Step 3: Make the request to the API
response <- GET(url, query = params)
# Step 4: Print the status code
print(paste("Response status code:", status_code(response)))
setwd("~/UW Study/Research/Spatiotemporal_CPB_Modeling/R")
library(tidyverse)
library(lme4)
# Optional: for ICC calculation
if (!require(performance)) install.packages("performance")
# Input/output folder paths
RAW_DATA_DIR <- "../data/raw/"
INTERIM_DATA_DIR <- "../data/interim/"
PROCESSED_DATA_DIR <- "../data/processed/"
data <- read.csv(file.path(PROCESSED_DATA_DIR, 'final_data_for_modeling.csv'))
print(dim(data))
# Create target: average CPB abundance
data$cpb_abundance <- (data$cpba_count + data$cpbl_count) / 2
data$croptype <- ifelse(data$croptype == 43, 1, 0)
# --- Scale numeric predictors
numeric_vars <- c(
"gdd", "cum_gdd", "wei_intensity", "wei_prop",
"summer_avg_temp", "summer_avg_percip", "summer_heavy_rainfall_days",
"summer_hottest_temp","summer_temp_variability", "winter_coldest_temp",
"winter_extreme_cold_days", "winter_temp_variability",
"winter_warm_day_count","winter_heavy_rainfall_days",  "spring_frost_free_days"
)
# Scale each numeric variable if it exists in data
for (v in numeric_vars) {
if (v %in% names(data)) data[[v]] <- scale(data[[v]])
}
nrow(data[data['croptype']==1,])
### LMER MODEL -----
# (a) Null vs. farm random effect
model_null  <- lm(cpb_abundance ~ 1, data = data)
model_farm  <- lmer(cpb_abundance ~ 1 + (1 | farm), data = data, REML = TRUE)
anova(model_farm,model_null)
# (b) Farm vs. farm+year random effects
model_farm_year <- lmer(cpb_abundance ~ 1 + (1 | farm) + (1 | year), data = data, REML = TRUE)
anova(model_farm, model_farm_year)
# --- Full mixed-effects model
full_model <- lmer(
cpb_abundance ~ gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days +
(1 | farm) + (1 | year),
data = data, REML = TRUE
)
summary(full_model)
# --- Setup
if (!require(mgcv)) {
install.packages("mgcv")
library(mgcv)
}
data$farm <- as.factor(data$farm)
data$year <- as.factor(data$year)
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days,
data = data, family = gaussian())
# Print the model summary and AIC
summary(gam_model)
# --- Setup
if (!require(spaMM)) {
install.packages("spaMM", repos = "https://pbil.univ-lyon1.fr/CRAN/")
library(spaMM)
}
spamm_SRF <- fitme(
cpb_abundance ~
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days +
Matern(1 | lat + lng) + (1 | year) + (1 | farm),
data = data, family = gaussian()
)
summary(spamm_SRF)
# Input/output folder paths
RAW_DATA_DIR <- "../data/raw/"
INTERIM_DATA_DIR <- "../data/interim/"
PROCESSED_DATA_DIR <- "../data/processed/"
data <- read.csv(file.path(PROCESSED_DATA_DIR, 'final_data_for_modeling.csv'))
print(dim(data))
# Create target: average CPB abundance
data$cpb_abundance <- (data$cpba_count + data$cpbl_count) / 2
data$croptype <- ifelse(data$croptype == 43, 1, 0)
# --- Scale numeric predictors
numeric_vars <- c(
"gdd", "cum_gdd", "wei_intensity", "wei_prop",
"summer_avg_temp", "summer_avg_percip", "summer_heavy_rainfall_days",
"summer_hottest_temp","summer_temp_variability", "winter_coldest_temp",
"winter_extreme_cold_days",
"winter_warm_day_count"
)
# Scale each numeric variable if it exists in data
for (v in numeric_vars) {
if (v %in% names(data)) data[[v]] <- scale(data[[v]])
}
nrow(data[data['croptype']==1,])
# (a) Null vs. farm random effect
model_null  <- lm(cpb_abundance ~ 1, data = data)
model_farm  <- lmer(cpb_abundance ~ 1 + (1 | farm), data = data, REML = TRUE)
anova(model_farm,model_null)
# (b) Farm vs. farm+year random effects
model_farm_year <- lmer(cpb_abundance ~ 1 + (1 | farm) + (1 | year), data = data, REML = TRUE)
anova(model_farm, model_farm_year)
# --- Full mixed-effects model
full_model <- lmer(
cpb_abundance ~ gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days +
(1 | farm) + (1 | year),
data = data, REML = TRUE
)
summary(full_model)
# --- Setup
if (!require(mgcv)) {
install.packages("mgcv")
library(mgcv)
}
# --- Full mixed-effects model
full_model <- lmer(
cpb_abundance ~ gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_warm_day_count +
(1 | farm) + (1 | year),
data = data, REML = TRUE
)
summary(full_model)
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_warm_day_count,
data = data, family = gaussian())
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days,
data = data, family = gaussian())
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days,
data = data, family = gaussian())
data <- read.csv(file.path(PROCESSED_DATA_DIR, 'final_data_for_modeling.csv'))
print(dim(data))
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days,
data = data, family = gaussian())
# Create target: average CPB abundance
data$cpb_abundance <- (data$cpba_count + data$cpbl_count) / 2
data$croptype <- ifelse(data$croptype == 43, 1, 0)
# --- Scale numeric predictors
numeric_vars <- c(
"gdd", "cum_gdd", "wei_intensity", "wei_prop",
"summer_avg_temp", "summer_avg_percip", "summer_heavy_rainfall_days",
"summer_hottest_temp","summer_temp_variability", "winter_coldest_temp",
"winter_extreme_cold_days",
"winter_warm_day_count"
)
# Scale each numeric variable if it exists in data
for (v in numeric_vars) {
if (v %in% names(data)) data[[v]] <- scale(data[[v]])
}
nrow(data[data['croptype']==1,])
# --- Setup
if (!require(mgcv)) {
install.packages("mgcv")
library(mgcv)
}
data$farm <- as.factor(data$farm)
data$year <- as.factor(data$year)
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_temp_variability + winter_warm_day_count + winter_heavy_rainfall_days + spring_frost_free_days,
data = data, family = gaussian())
# Print the model summary and AIC
summary(gam_model)
gam_model <- gam(cpb_abundance ~ s(lat, lng) + s(year, bs = "re") + s(farm, bs = "re") +
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_warm_day_count,
data = data, family = gaussian())
# Print the model summary and AIC
summary(gam_model)
# --- Setup
if (!require(spaMM)) {
install.packages("spaMM", repos = "https://pbil.univ-lyon1.fr/CRAN/")
library(spaMM)
}
spamm_SRF <- fitme(
cpb_abundance ~
gdd + cum_gdd + croptype + wei_intensity + wei_prop +
summer_avg_temp + summer_avg_percip + summer_heavy_rainfall_days +
summer_temp_variability + summer_hottest_temp + winter_coldest_temp + winter_extreme_cold_days +
winter_warm_day_count +
Matern(1 | lat + lng) + (1 | year) + (1 | farm),
data = data, family = gaussian()
)
summary(spamm_SRF)
